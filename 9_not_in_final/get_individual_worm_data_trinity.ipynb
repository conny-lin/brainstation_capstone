{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove existing files\n",
      "taking first 0 sample files for testing\n",
      "2001 files to process\n",
      "4300854 rows for the test_trinity_data.csv output\n",
      "start timer\n"
     ]
    }
   ],
   "source": [
    "# Get trinity worm data from 0mM and 400mM N2\n",
    "# Conny Lin | June 2, 2020\n",
    "# connylin@doctor.com\n",
    "# \n",
    "# concat trinity.wormid.dat files into one csv file\n",
    "# add worm id and ethanol groups (0 = no ethanol, 1 = 400mM ethanol)\n",
    "# save the file\n",
    "# !BUG! if sample size is set to 2000, save to csv doesn't seem to work\n",
    "\n",
    "# import libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# set test sample size (if 0, will run the complete file list)\n",
    "test_sample_size_random = 0 #!BUG! \n",
    "test_sample_size_sequential = 2000\n",
    "test_sample_size = test_sample_size_random+test_sample_size_sequential\n",
    "output_filename = 'test_trinity_data.csv'\n",
    "output_temp_trinity = 'temp_trinity.csv'\n",
    "output_temp_id = 'temp_wormid_eth.csv'\n",
    "output_db = 'test_trinity_db.pickle'\n",
    "dir_save = '/Users/connylin/Dropbox/CA/ED _20200119 Brain Station Data Science Diploma/Capstone/data'\n",
    "random_number = 1\n",
    "\n",
    "\n",
    "# delete temp files if exist\n",
    "print('remove existing files')\n",
    "temp_files = [output_filename, output_temp_trinity, output_temp_id]\n",
    "for fn in temp_files:\n",
    "    p = os.path.join(dir_save, fn)\n",
    "    if os.path.exists(p):\n",
    "        os.remove(p)\n",
    "        \n",
    "# get file info file --\n",
    "p = os.path.join(dir_save, 'fileinfo_trinity_N2400mM.pickle')\n",
    "fileinfoDB = pickle.load(open(p,'rb'))\n",
    "filenumber_allfiles = fileinfoDB.shape[0]\n",
    "\n",
    "# get a sample size or the full set\n",
    "if test_sample_size_random > 0:\n",
    "    print(f'taking random {test_sample_size_random} sample files for testing')\n",
    "    df_target = fileinfoDB.sample(test_sample_size_random, random_state=random_number).copy()\n",
    "elif test_sample_size_sequential > 0:\n",
    "    print(f'taking first {test_sample_size_random} sample files for testing')\n",
    "    df_target = fileinfoDB.iloc[:test_sample_size_sequential+1,:].copy()\n",
    "else:\n",
    "    print(f'processing the full set of files')\n",
    "    df_target = fileinfoDB\n",
    "print(f'{df_target.shape[0]} files to process')\n",
    "row_total = df_target['row_number'].sum()\n",
    "print(f'{row_total} rows for the {output_filename} output')\n",
    "del fileinfoDB\n",
    "\n",
    "# write database\n",
    "p = os.path.join(dir_save, output_db)\n",
    "fileinfoDB = pickle.dump(df_target, open(p,'wb'))\n",
    "\n",
    "# start timer\n",
    "print('start timer')\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating worm id and ethanol id array\t\t\t\t\t0.00min elapsed\n",
      "shapes of worm id: 4300854\n",
      "saving worm id and ethanol id array into csv\t\t\t\t0.01min elapsed\n",
      "id array size:\t\t\t\t\t\t\t\t29 MB\n",
      "estimate trinity array size:\t\t\t\t\t\t371 MB\n"
     ]
    }
   ],
   "source": [
    "# -- create wormid array and ethanol array --\n",
    "# get info\n",
    "worm_id_list = df_target['worm_id'].values\n",
    "ethanol_list = df_target['ethanol'].values\n",
    "rows_list = df_target['row_number'].values\n",
    "# instantiate arrays\n",
    "worm_id_array = np.empty([row_total],dtype='int')\n",
    "ethanol_array = np.empty([row_total],dtype='int')\n",
    "# create arrays\n",
    "print(f'creating worm id and ethanol id array\\t\\t\\t\\t\\t{(time.time() - start_time)/60:.2f}min elapsed')\n",
    "i_row_previous = 0\n",
    "for worm_id, ethanol, rows in zip(worm_id_list, ethanol_list, rows_list):\n",
    "    # get start row position\n",
    "    i_start_row = i_row_previous\n",
    "    # get end row position\n",
    "    i_end_row = i_row_previous+rows-1\n",
    "    # get worm id array x number of rows\n",
    "    worm_id_array[i_start_row:i_end_row+1] = np.tile(worm_id, rows)\n",
    "    # get ethanol array x number of rows\n",
    "    ethanol_array[i_start_row:i_end_row+1] = np.tile(ethanol, rows)\n",
    "    # create next starting position\n",
    "    i_row_previous = rows+i_row_previous\n",
    "# check output\n",
    "print(f'shapes of worm id: {worm_id_array.shape[0]}')\n",
    "# save to space deliminated without header --\n",
    "print(f'saving worm id and ethanol id array into csv\\t\\t\\t\\t{(time.time() - start_time)/60:.2f}min elapsed')\n",
    "# create data frame\n",
    "df = pd.DataFrame({'worm_id':worm_id_array, \n",
    "                   'ethanol':ethanol_array})\n",
    "# save csv #!BUG! \n",
    "df.to_csv(os.path.join(dir_save, output_temp_id), \n",
    "          sep=' ',index=False, header=False)\n",
    "# get size of output\n",
    "size_id_array_mb = (os.path.getsize(os.path.join(dir_save, output_temp_id))) \n",
    "print(f'id array size:\\t\\t\\t\\t\\t\\t\\t\\t{size_id_array_mb/1000**2:.0f} MB')\n",
    "print(f'estimate trinity array size:\\t\\t\\t\\t\\t\\t{(size_id_array_mb/1000**2)*df_target.shape[1]:.0f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linking file path\n",
      "trinity array size:\t\t\t\t\t\t\t0 MB\n"
     ]
    }
   ],
   "source": [
    "# -- concat all trinity files --\n",
    "# link file paths\n",
    "print('linking file path')\n",
    "bashcommand_sourcepaths = ''\n",
    "for p in df_target['path'].values:\n",
    "    p = '\"'+p+'\"'\n",
    "    bashcommand_sourcepaths += p\n",
    "    bashcommand_sourcepaths += ' '\n",
    "# create bash command components\n",
    "bashcommand_prefix = 'cat'\n",
    "bashcommand_space = ' '\n",
    "bashcommand_link = '>'\n",
    "bashcommand_outputpath = '\"'+os.path.join(dir_save, output_temp_trinity)+'\"'\n",
    "# create bash command\n",
    "bashcommand = 'cat ' + bashcommand_sourcepaths + '> ' + bashcommand_outputpath\n",
    "# call bash command\n",
    "os.system(bashcommand)\n",
    "# report output size\n",
    "size_trinity = (os.path.getsize(os.path.join(dir_save, output_temp_trinity)))\n",
    "print(f'trinity array size:\\t\\t\\t\\t\\t\\t\\t{size_trinity/1000**2:.0f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash concatenate worm id, ethanol id array with trinity.dat\t\t0.18min elapsed\n",
      "-------\n",
      "total processing time: \t\t\t\t\t\t\t0.28min elapsed\n",
      "estimate processing time for full set:\t\t\t\t\t29min\n",
      "estimate trinity file size for full set:\t\t\t\t0GB\n"
     ]
    }
   ],
   "source": [
    "# -- bash concat id array and trinity array --\n",
    "print(f'bash concatenate worm id, ethanol id array with trinity.dat\\t\\t{(time.time() - start_time)/60:.2f}min elapsed')\n",
    "bashcommand = \"paste '\" + \\\n",
    "            os.path.join(dir_save, output_temp_id) + \\\n",
    "            \"' '\" + \\\n",
    "            os.path.join(dir_save, output_temp_trinity) + \\\n",
    "            \"' > '\" + \\\n",
    "            os.path.join(dir_save, output_filename) + \\\n",
    "            \"'\"\n",
    "os.system(bashcommand)\n",
    "\n",
    "# report estimate time\n",
    "processing_time =(time.time() - start_time)/60\n",
    "print(f'-------\\ntotal processing time: \\t\\t\\t\\t\\t\\t\\t{processing_time:.2f}min elapsed')\n",
    "print(f'estimate processing time for full set:\\t\\t\\t\\t\\t{filenumber_allfiles/test_sample_size*processing_time:.0f}min')\n",
    "print(f'estimate trinity file size for full set:\\t\\t\\t\\t{(filenumber_allfiles/test_sample_size)*(size_trinity/1000**3):.0f}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test load\n",
    "col_keep_ind_legend = np.array([0,3,5,6,8,9,10,11,12,13,14,15,16,17])\n",
    "col_keep_ind_output = np.hstack((np.array([0,1]), col_keep_ind_legend+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make coluumn name\n",
    "p = os.path.join(dir_save, 'legend_trinity_worm.pickle')\n",
    "cnames = pickle.load(open(p,'rb'))\n",
    "cnames = cnames.loc[col_keep_ind_legend,'name'].values\n",
    "cnames = np.hstack((np.array(['worm_id','ethanol']),cnames)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Too many columns specified: expected 16 and found 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1af81b653a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_keep_ind_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                  names=cnames)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Too many columns specified: expected 16 and found 2"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(dir_save, output_filename), \n",
    "                 delim_whitespace=True,\n",
    "                 header=None,\n",
    "                 usecols=col_keep_ind_output, \n",
    "                 names=cnames)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

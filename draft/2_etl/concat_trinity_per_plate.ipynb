{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trinity worm data from 0mM and 400mM N2\n",
    "# Conny Lin | June 3, 2020\n",
    "# connylin@doctor.com\n",
    "#\n",
    "# combine all trinity within a plate into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = '/Users/connylin/Dropbox/CA/ED _20200119 Brain Station Data Science Diploma/Capstone/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get links to each plate --\n",
    "# load pickle\n",
    "plateDB = pickle.load(open(os.path.join(dir_save, 'file_summary_mwt.pickle'),'rb'))\n",
    "# get paths with trinity.id.dat\n",
    "pMWT = plateDB.index[~plateDB[('filepath','trinity.id.dat')].isna()].values\n",
    "\n",
    "col_ind_keep = [0,3,5,6,8,9,10,11,12,13,14,15,16,17]\n",
    "# load column names legend\n",
    "p = os.path.join(dir_save, 'legend_trinity_worm.pickle')\n",
    "cnames = pickle.load(open(p,'rb'))\n",
    "cnames = cnames['name'].values\n",
    "cnames_keep = cnames[col_ind_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 88/871 plate\n",
      "\t2501 trinity files in this plate\n",
      "\tslice process trinity: /Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity0.dat\n",
      "\tslice process trinity: /Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity1.dat\n",
      "\tslice process trinity: /Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity2.dat\n",
      "cat '/Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity0.dat' '/Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity1.dat' '/Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity2.dat' > '/Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity.dat'\n",
      "\tcombine slice processed trinity\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-bb92ae7e8b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# load tirnity concated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsave_trinity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_ind_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnames_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;31m# add wormid field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wormid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_repeats_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'list'>"
     ]
    }
   ],
   "source": [
    "# go in each plate\n",
    "# for p in pMWT:\n",
    "starting_plate = 87\n",
    "for plate_count, pPlate in enumerate(pMWT[starting_plate:],starting_plate+1):\n",
    "    print(f'processing {plate_count}/{pMWT.shape[0]} plate')\n",
    "    # find all trinity files\n",
    "    ptri = glob.glob(pPlate+\"/*.trinity.*.dat\")\n",
    "    # print number of files\n",
    "    print(f'\\t{len(ptri)} trinity files in this plate')\n",
    "\n",
    "\n",
    "    # get worm id from filep path\n",
    "    df = pd.DataFrame({'paths':ptri})\n",
    "    worm_number = df['paths'].str.extract(r'(?<=trinity[.])(\\d{1,})')\n",
    "    worm_number = worm_number.astype('int32').values\n",
    "\n",
    "    # get number of rows per doc\n",
    "    # declare empty array to store the row numbers\n",
    "    rows_array = np.empty((len(ptri)),dtype='int')\n",
    "    # loop through each files \n",
    "    for i, p in enumerate(ptri, 0):\n",
    "        # report process every 100 files\n",
    "        if (i%100==0):\n",
    "            print(f'\\tprocessing #{i} files', end='\\r')\n",
    "        # create custom bash command\n",
    "        bashCommand = \"cat \"+p+\" | wc -l\"\n",
    "        # call bash\n",
    "        output = subprocess.check_output(bashCommand, shell=True)\n",
    "        # take out digits from the byte output\n",
    "        rows_array[i] = int(output)\n",
    "\n",
    "\n",
    "    # create worm id array\n",
    "    # get info\n",
    "    row_total = rows_array.sum()\n",
    "    # instantiate arrays\n",
    "    row_repeats_array = np.empty([row_total], dtype='int')\n",
    "    # create arrays\n",
    "    i_row_previous = 0\n",
    "    for worm_id, rows in zip(worm_number, rows_array):\n",
    "        # get start row position\n",
    "        i_start_row = i_row_previous\n",
    "        # get end row position\n",
    "        i_end_row = i_row_previous+rows-1\n",
    "        # get worm id array x number of rows\n",
    "        row_repeats_array[i_start_row:i_end_row+1] = np.tile(worm_id, rows)\n",
    "        # create next starting position\n",
    "        i_row_previous = rows+i_row_previous\n",
    "    \n",
    "    \n",
    "    # concat paths (process by 1000/batch)\n",
    "    # copy path file for trimming\n",
    "    ptri_bash = ptri.copy()\n",
    "    psave_trinity_list = []\n",
    "    # while pathfile still has stuff in it\n",
    "    while len(ptri_bash) > 0:\n",
    "        ptri_len = len(ptri_bash)\n",
    "        ptri_bash_process = ptri_bash[:1000].copy()\n",
    "        ptri_bash = ptri_bash[1000:].copy()\n",
    "\n",
    "        bashcommand_sourcepaths = ''\n",
    "        for p in ptri_bash_process:\n",
    "            p = \"'\"+p+\"'\"\n",
    "            bashcommand_sourcepaths += p\n",
    "            bashcommand_sourcepaths += ' '\n",
    "        # create bash command components\n",
    "        n_files = len(psave_trinity_list)\n",
    "        psave_trinity = os.path.join(pPlate, f'temp_trinity{n_files}.dat')\n",
    "        bashcommand = 'cat ' + bashcommand_sourcepaths + '> ' + \"'\"+ psave_trinity + \"'\"\n",
    "        psave_trinity_list.append(psave_trinity)\n",
    "        print(f'\\tslice process trinity: {psave_trinity}')\n",
    "        # call bash command\n",
    "        os.system(bashcommand)\n",
    "\n",
    "    # cat temp trinity files\n",
    "    psave_trinity = os.path.join(pPlate, 'temp_trinity.dat')\n",
    "    if len(psave_trinity_list)>0:\n",
    "        bashcommand_sourcepaths = ''\n",
    "        for p in psave_trinity_list:\n",
    "            p = \"'\"+p+\"'\"\n",
    "            bashcommand_sourcepaths += p\n",
    "            bashcommand_sourcepaths += ' '\n",
    "        bashcommand = 'cat ' + bashcommand_sourcepaths + '> ' + \"'\"+ psave_trinity + \"'\"\n",
    "        os.system(bashcommand)\n",
    "        # remove files\n",
    "        for p in psave_trinity_list:\n",
    "            os.remove(p)\n",
    "        print('\\tcombine slice processed trinity')\n",
    "    \n",
    "\n",
    "    # load tirnity concated\n",
    "    df = pd.read_csv(psave_trinity, delim_whitespace=True, header=None, usecols=col_ind_keep, names=cnames_keep)\n",
    "    # add wormid field\n",
    "    df['wormid'] = row_repeats_array\n",
    "\n",
    "    # into pickle\n",
    "    psave_pickle = os.path.join(pPlate, 'trinity_all_worms.pickle')\n",
    "    pickle.dump(df, open(psave_pickle,'wb'))\n",
    "    if os.path.exists(psave_pickle):\n",
    "        size_trinity = os.path.getsize(psave_pickle)\n",
    "        print(f'\\tsize of pickle file: {size_trinity/1000**2:.0f} MB')\n",
    "        if os.path.exists(psave_trinity):\n",
    "            print('\\tclean up trinity temp file')\n",
    "            os.remove(psave_trinity)\n",
    "    else:\n",
    "        print('\\tpickle file did not save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity0.dat',\n",
       " '/Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity1.dat',\n",
       " '/Volumes/COBOLT/MWT/20120309X_CL_100s30x60s10s/NM1968/20120309_185850/temp_trinity2.dat']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psave_trinity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
